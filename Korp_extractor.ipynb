{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import json\n",
    "import requests \n",
    "from korp.korp import Korp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give substring to search for. For example \"talou.\" which gets words text involving words like taloudellinen, talous\n",
    "word_or_substring = 'omena.'\n",
    "\n",
    "# Give the amount of answers to get\n",
    "amount_of_texts_to_get = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define url parameters\n",
    "definition = {\n",
    "'defaultcontext'\t:\"=1+paragraph\",\n",
    "'defaultwithin'\t\t:\"=sentence\",\n",
    "'show'\t\t\t\t:\"=sentence,paragraph,lemma,pos,msd,dephead,deprel,ref,lex,nertag\",\n",
    "'show_struct'\t\t:\"=text_title,text_date,text_time,text_sect,text_sub,text_user,sentence_id,text_urlmsg,text_urlboard\",\n",
    "'start'\t\t\t\t:\"=0\",\n",
    "'end'\t\t\t\t:\"={}\".format(amount_of_texts_to_get), \n",
    "'corpus'\t\t\t:\"=S24\",\n",
    "'cqp'\t\t\t\t:\"=%5Blemma+%3D+%22{}*%22%5D\".format(word_or_substring),\n",
    "'indent'\t\t\t:\"=2\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets construct url\n",
    "URL= 'https://korp.csc.fi/cgi-bin/korp.cgi?command=query&'\n",
    "for k,v in definition.items():\n",
    "    if k == str(list(definition.keys())[-1]):\n",
    "        URL = URL + str(k) + str(v)\n",
    "    else:URL = URL + str(k) + str(v) +'&'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sending get request and saving the response as response object \n",
    "r = requests.get(url = URL)\n",
    "  \n",
    "# extracting data in json format \n",
    "data = r.json() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets get rid of some unneccessary part of the data\n",
    "data= data['kwic']\n",
    "# Lets now normalize the data to right format\n",
    "# It now gets one word per row so not yeat perfect but we will transform data more later\n",
    "data = pd.io.json.json_normalize(data,\n",
    "                                  record_path = 'tokens',\n",
    "                                  meta=[\n",
    "                                      ['structs', 'sentence_id'],\n",
    "                                      ['structs', 'text_sect'],\n",
    "                                      ['structs', 'text_sub'],\n",
    "                                      ['structs', 'text_time'],\n",
    "                                      ['structs', 'text_title']\n",
    "                                       ])#,errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take out unneccessary columns\n",
    "dataframe_clean = data[[\n",
    "'structs.sentence_id',\n",
    "'structs.text_sect',\n",
    "'structs.text_sub',\n",
    "'structs.text_time',\n",
    "'structs.text_title',\n",
    "'word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>structs.sentence_id</th>\n",
       "      <th>structs.text_sect</th>\n",
       "      <th>structs.text_sub</th>\n",
       "      <th>structs.text_time</th>\n",
       "      <th>structs.text_title</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15277</td>\n",
       "      <td>Ruoka ja juoma</td>\n",
       "      <td>Vegaaniruoka</td>\n",
       "      <td>20:52</td>\n",
       "      <td>Jouluruoat, juomat ja herkut</td>\n",
       "      <td>Sinun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15277</td>\n",
       "      <td>Ruoka ja juoma</td>\n",
       "      <td>Vegaaniruoka</td>\n",
       "      <td>20:52</td>\n",
       "      <td>Jouluruoat, juomat ja herkut</td>\n",
       "      <td>kannattaisi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15277</td>\n",
       "      <td>Ruoka ja juoma</td>\n",
       "      <td>Vegaaniruoka</td>\n",
       "      <td>20:52</td>\n",
       "      <td>Jouluruoat, juomat ja herkut</td>\n",
       "      <td>ostaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15277</td>\n",
       "      <td>Ruoka ja juoma</td>\n",
       "      <td>Vegaaniruoka</td>\n",
       "      <td>20:52</td>\n",
       "      <td>Jouluruoat, juomat ja herkut</td>\n",
       "      <td>kunnon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15277</td>\n",
       "      <td>Ruoka ja juoma</td>\n",
       "      <td>Vegaaniruoka</td>\n",
       "      <td>20:52</td>\n",
       "      <td>Jouluruoat, juomat ja herkut</td>\n",
       "      <td>kasvisruokakirja</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  structs.sentence_id structs.text_sect structs.text_sub structs.text_time  \\\n",
       "0               15277    Ruoka ja juoma     Vegaaniruoka             20:52   \n",
       "1               15277    Ruoka ja juoma     Vegaaniruoka             20:52   \n",
       "2               15277    Ruoka ja juoma     Vegaaniruoka             20:52   \n",
       "3               15277    Ruoka ja juoma     Vegaaniruoka             20:52   \n",
       "4               15277    Ruoka ja juoma     Vegaaniruoka             20:52   \n",
       "\n",
       "             structs.text_title              word  \n",
       "0  Jouluruoat, juomat ja herkut             Sinun  \n",
       "1  Jouluruoat, juomat ja herkut       kannattaisi  \n",
       "2  Jouluruoat, juomat ja herkut             ostaa  \n",
       "3  Jouluruoat, juomat ja herkut            kunnon  \n",
       "4  Jouluruoat, juomat ja herkut  kasvisruokakirja  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check how data looks now\n",
    "dataframe_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets fill empty values with __empty__\n",
    "dataframe_clean = dataframe_clean.fillna(value= '__empty__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data so we get one message per row\n",
    "data_test = pd.DataFrame(dataframe_clean.groupby(['structs.sentence_id',\n",
    "                                      'structs.text_sect',\n",
    "                                      'structs.text_sub',\n",
    "                                      'structs.text_time',\n",
    "                                      'structs.text_title'])['word'].apply(','.join))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets tokenize data, if you dont have neeeded packages, do the following:\n",
    "# install nltk with pip\n",
    "# got to command line in this env, then run python\n",
    "# Then in python run the following:\n",
    "# import nltk\n",
    "# nltk.download() --> This should pop up download window from which you can download needed packages\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data_test['tokenized'] = 'sanoja'\n",
    "for row_num in range(len(data_test)):\n",
    "    tokenized = tokenizer.tokenize(data_test['word'].iloc[row_num])\n",
    "    data_test['tokenized'].iloc[row_num] = tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Amount of unique messages in data {} kpl \".format(data_test['structs.sentence_id'].nunique()))\n",
    "#print(data_test.groupby('structs.text_sub').size())\n",
    "#print(data_test.groupby('structs.text_sect').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as csv\n",
    "data_test.to_csv(\"data/{}.csv\".format(word_or_substring))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:FinBertTrainEnv] *",
   "language": "python",
   "name": "conda-env-FinBertTrainEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
